# 289_IntroToML

the homework and final project for EECS289 in UC Berkeley

HW0: Review and Linear Regression
* Q3:  investigate how the presence of noise in the data can adversely affect the model that we learn from it.

HW1: Statistics and Least Squares
* Q2, Q3: statistical problems
* Q4: System Identification by ordinary least squares regression
* Q5: A Simple Classification Approach by using linear regression

HW2: Ridge Regression, cross-validation
* Q2: Geometry of Ridge Regression
* Q3: Polynomials and invertibility
* Q4: Polynomials and approximation
* Q5: Jaina and her giant peaches - fit a polynomial of degree D to this data with linear regression
* Q6: Nonlinear Classification Boundaries - use polynomial features to learn nonlinear classification boundaries.

HW3: Bias-Variance Tradeoff
* Q2: Probabilistic Model of Linear Regression
* Q3: Simple Bias-Variance Tradeoff
* Q4: Estimation and approximation in linear regression - explore the quality of polynomial regressions when the true function is not polynomial.
* Q5: Robotic Learning of Controls from Demonstrations and Images

HW4: Kernel Ridge Regression
* Q2: MLE of Multivariate Gaussian
* Q3: Tikhonov Regularization and Weighted Least Squares
* Q4: Kernel Ridge Regression - Theory
* Q5: Kernel Ridge Regression - Practice

HW5: TLS and PCA
* Q2: Total Least Squares
* Q3: PCA and Random Projections

HW6: CCA
* Q2:  CCA - finding a solution to the problem of canonical correlation analysis using the singular value decomposition.
* Q3:  [Mooney Reconstruction](https://github.com/JIASIYAO/289_IntroToML/blob/master/hw6/Mooney_Reconstruction/Mooney%20Reconstruction.ipynb) - restore photos of celebrities from Mooney photos, which are bina- rized faces

HW7: Gradient Descent
* Q2: Step Size in Gradient Descent
* Q3: Convergence Rate of Gradient Descent
* Q4: Sensors, Objects, and Localization - using gradient descent to solve the problem of figuring out where objects are, given noisy distance measurements.

HW8: SGD
* Q2: SGD on OLS
* Q3: Gradient Descent Framework -  using classification models such as logistic regression and neural networks, and solve it using stochastic gradient descent
* Q4: Genome-Wide Association Study

HW9: LDA and QDA
* Q2: Classification Policy
* Q3: LDA and CCA
* Q4: Sensors, Objects, and Localization (Part 2)
* Q5: Entropy, KL Divergences, and Cross-Entropy

HW10: Classification
* Q2: Regularized and Kernel k-Means
* Q3: Linear Methods on Fruits and Veggies
* Q4: Expectation Maximization (EM) Algorithm: A closer look!

HW11: SVM
* Q2: SVM with custom margins
* Q3: Nearest Neighbors, from A to Z
* Q4: A Unified Approach to Generalization for Classification and Regression

HW12: Random Forest
* Q2: l1-Regularized Linear Regression: LASSO
* Q3: Variance of Sparse Linear Models Obtained by Thresholding
* Q4: Decision Trees and Random Forests

HW13: Boosting and CNN
* Q2: Gradient boosting and early stopping
* Q3: CNNs on Fruits and Veggies
* Q4: Running Time of k-Nearest Neighbour Search Methods

Final Project: [Learning Many Body Wavefunction](https://github.com/JIASIYAO/289_IntroToML/blob/master/final_project/wave_function_neural_net.ipynb)
